{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "name": "homework4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-__q1WNXV1Zc"
      },
      "source": [
        ""
      ],
      "id": "-__q1WNXV1Zc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7deae09c-da1d-4d16-8551-b00dd566e8f1"
      },
      "source": [
        "# About python imports\n",
        "\n",
        "No imports are specified as in previous homeworks, please insert the necessary python code yourself."
      ],
      "id": "7deae09c-da1d-4d16-8551-b00dd566e8f1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "608c1096-a095-4731-b9e9-df81c9f59c22"
      },
      "source": [
        "-----\n",
        "# Exercise 1 (3 points)\n",
        "Write a program implementing Rayleigh quotient iteration for computing an eigenvalue and corresponding eigenvector of a matrix. Test your program on the matrix \n",
        "$$\n",
        "  A = \\begin{bmatrix} 6 & 2 & 1 \\\\ 2& 3 & 1 \\\\ 1 & 1 & 1  \\end{bmatrix}\n",
        "$$\n",
        "using a random starting vector. Let the program create output that shows the convergence behavior."
      ],
      "id": "608c1096-a095-4731-b9e9-df81c9f59c22"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72c19f0e-4aea-4b93-b4c4-44b3b78b4443"
      },
      "source": [
        "import numpy as np\n",
        "def ray(A, x0, iterations):\n",
        "  xold = x0\n",
        "  I = np.identity(np.shape(A)[0])\n",
        "  numerator = np.matmul(np.matmul(np.transpose(xold), A), xold)\n",
        "  denomenator = np.matmul(np.transpose(xold), xold)\n",
        "  sigma = numerator/denomenator\n",
        "  print(sigma)\n",
        "  for i in range(iterations):\n",
        "    ynew = np.linalg.solve(A-(sigma * I), xold)\n",
        "    xnew = ynew/np.linalg.norm(ynew, ord=np.inf)\n",
        "    xold = xnew\n",
        "    numerator = np.matmul(np.matmul(np.transpose(xnew), A), xnew)\n",
        "    denomenator = np.matmul(np.transpose(xnew), xnew)\n",
        "    sigma = numerator/denomenator\n",
        "    print(sigma)\n",
        "    print(xnew)\n",
        "  \n",
        "\n",
        "  \n",
        "  return sigma, xnew\n",
        "\n",
        "\n"
      ],
      "id": "72c19f0e-4aea-4b93-b4c4-44b3b78b4443",
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqRcJIvHEMMF",
        "outputId": "c15d3d77-5312-403b-a2ea-ab9e4bcc3da5"
      },
      "source": [
        "A = np.array([[6,2,1], [2,3,1], [1,1,1]])\n",
        "eigval, eigvec = ray2(A, np.transpose(np.array((1,1,1))), 10)\n",
        "np.linalg.eig(A)"
      ],
      "id": "HqRcJIvHEMMF",
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.0\n",
            "7.153846153846153\n",
            "[1.  0.4 0.1]\n",
            "[ 1.   0.4 -0.8]\n",
            "7.287908163680387\n",
            "[1.         0.5266558  0.24572227]\n",
            "[1.         0.52979661 0.2716541 ]\n",
            "7.287992138960401\n",
            "[1.         0.5229001  0.24219175]\n",
            "[1.         0.52290002 0.24219142]\n",
            "7.287992138960422\n",
            "[1.         0.52290017 0.24219181]\n",
            "[1.         0.52290017 0.24219181]\n",
            "7.287992138960422\n",
            "[1.         0.52290017 0.24219181]\n",
            "[1.         0.52290017 0.24219181]\n",
            "7.287992138960422\n",
            "[1.         0.52290017 0.24219181]\n",
            "[1.         0.52290017 0.24219181]\n",
            "7.287992138960422\n",
            "[1.         0.52290017 0.24219181]\n",
            "[1.         0.52290017 0.24219181]\n",
            "7.287992138960422\n",
            "[1.         0.52290017 0.24219181]\n",
            "[1.         0.52290017 0.24219181]\n",
            "7.287992138960422\n",
            "[1.         0.52290017 0.24219181]\n",
            "[1.         0.52290017 0.24219181]\n",
            "7.287992138960422\n",
            "[1.         0.52290017 0.24219181]\n",
            "[1.         0.52290017 0.24219181]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([7.28799214, 2.13307448, 0.57893339]),\n",
              " array([[ 0.86643225,  0.49742503, -0.0431682 ],\n",
              "        [ 0.45305757, -0.8195891 , -0.35073145],\n",
              "        [ 0.20984279, -0.28432735,  0.9354806 ]]))"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VglTq7HoIOg7",
        "outputId": "c95a9d5a-d1b0-4c97-9dc5-f93a71f2a2b0"
      },
      "source": [
        "print(np.dot(A, eigvec))\n",
        "np.dot(eigval, eigvec)"
      ],
      "id": "VglTq7HoIOg7",
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7.28799214 3.81089231 1.76509197]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7.28799214, 3.81089231, 1.76509197])"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "ae4fb906-e2a8-44c6-94ed-f4c1c69f8105"
      },
      "source": [
        "# Automatic differentiation using JAX\n",
        "\n",
        "In applications of rootfinding, computing the derivative is often a problematic step. For example, the function for which zeros are sought might be given by a complicated computer program. \n",
        "\n",
        "Automatic differentiation is a set of techniques to evaluate the derivative of a function specified by a computer program. See the [wikipedia page](https://en.wikipedia.org/wiki/Automatic_differentiation) on this topic.\n",
        "\n",
        "[JAX](https://github.com/google/jax) is a software package that implements automatic differentiation as well as other functionality. [The documentation is here](https://jax.readthedocs.io/en/latest/). The idea of this exercise is to use JAX for obtaining derivative functions.\n",
        "\n",
        "To use JAX, there are two options:\n",
        "- install JAX. The installation of JAX is described on the Github page, see https://github.com/google/jax#installation.  **It appears that installation under Windows is not supported. According to the internet, one may use the Windows Subsystem for Linux, but I haven't tested this.**\n",
        "- run your python notebook on the google colab environment. The google colab environment is at https://colab.research.google.com. In the google colab environment, the JAX package is available. \n",
        "\n",
        "There is some material online about JAX, see for example\n",
        "https://medium.com/swlh/solving-optimization-problems-with-jax-98376508bd4f\n",
        "(LATEX-pdf version here\n",
        "https://github.com/mazy1998/Solving-Optimization-Problems-with-JAX/blob/master/Opitimization_with_jax.pdf)\n",
        "or \n",
        "https://www.kaggle.com/aakashnain/tf-jax-tutorials-part1.\n",
        "\n",
        "The result is that for many functions, the derivative can be automatically computed. We will show this for a vector valued function $\\mathbb{R}^2 \\to \\mathbb{R}^2$. \n",
        "\n",
        "The first step is to import some functions from the package JAX. Notice that JAX has its own version of numpy. Here we import it as `jnp`."
      ],
      "id": "ae4fb906-e2a8-44c6-94ed-f4c1c69f8105"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "2941c681-32c8-4d6a-b941-cd1ca6a8ed55"
      },
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "from jax import jacfwd, jacrev \n",
        "\n",
        "# depending on the application, more imports are needed"
      ],
      "id": "2941c681-32c8-4d6a-b941-cd1ca6a8ed55",
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cd7b92c-1234-4e33-8327-73427a0edcab"
      },
      "source": [
        "JAX implements forward and reverse mode automatic differentiation. The commands are `jacfwd` and `jacrev` (jac stands for Jacobian). The [wikipedia page on automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) briefly introduces forward and reverse mode automatic differentiation. Here we just mention that forward accumulation is more efficient than reverse accumulation for functions $f : \\mathbb{R}^n \\to \\mathbb{R}^m$ with $m \\gg n$ as only $n$ sweeps are necessary, compared to $m$ sweeps for reverse accumulation and that reverse accumulation is more efficient than forward accumulation for functions $f : \\mathbb{R}^n \\to \\mathbb{R}^m$ with $m \\ll n$ as only $n$ sweeps are necessary.\n",
        "\n",
        "In the following cell a simple function is defined and differentiated. Note that JAX has its own array type, `jax.numpy.array` (because of the line `import jax.numpy as jnp` we write this as `jnp.array`), that output is in 32 bits floating point format in a different array type `DeviceArray` (JAX has a preference for the 32 bits floating point format and you are allowed to use it in this exercise). "
      ],
      "id": "8cd7b92c-1234-4e33-8327-73427a0edcab"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afa2d99c-adcd-45c6-a786-9653b0d2a390",
        "outputId": "b679bf48-e64d-4754-a018-3f3f58a45577"
      },
      "source": [
        "def circle(x): return x[0]**2 + x[1]**2\n",
        "J = jacfwd(circle)\n",
        "J(jnp.array([1.0 ,2.0]))"
      ],
      "id": "afa2d99c-adcd-45c6-a786-9653b0d2a390",
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([2., 4.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1fbe136-c903-4a13-867d-b6738275a2bc"
      },
      "source": [
        "It is allowed to take derivatives of derivatives, so that a second derivative matrix can be obtained."
      ],
      "id": "b1fbe136-c903-4a13-867d-b6738275a2bc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f909ba17-e90f-44d1-a79b-0c1e2e03a84f",
        "outputId": "1a193661-0553-4dff-a87c-31ead94f814f"
      },
      "source": [
        "def hessian(f): return jacfwd(jacrev(f))\n",
        "H = hessian(circle)\n",
        "myMatrix = H(jnp. array ([1.0 ,2.0]) )\n",
        "print(myMatrix)"
      ],
      "id": "f909ba17-e90f-44d1-a79b-0c1e2e03a84f",
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2. 0.]\n",
            " [0. 2.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61c586ff-da85-4772-8596-e5558a95a6b3"
      },
      "source": [
        "Although this is not the standard `numpy.ndarray` format, it appears however that this format can be used in linear algebra operations such as solve."
      ],
      "id": "61c586ff-da85-4772-8596-e5558a95a6b3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43cbca1e-3c02-47ce-aeee-bcac0dd35443",
        "outputId": "c39a16b0-0263-461e-cb78-c70724b37dfe"
      },
      "source": [
        "myVector = jnp.array([0.5, 2.0])\n",
        "\n",
        "import scipy.linalg as la\n",
        "la.solve(myMatrix, myVector)"
      ],
      "id": "43cbca1e-3c02-47ce-aeee-bcac0dd35443",
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.25, 1.  ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0488e28-7322-46ca-a2a3-6f2b9e8e81d0"
      },
      "source": [
        "It is easy to define vector valued functions, by returning an `jax.numpy.array` object. \n",
        "\n",
        "When using functions such as $\\sin$ and $\\cos$ and $\\exp$ one must be careful. One must use the functions `jax.numpy.sin`, `jax.numpy.cos`, etc. (and not `math.sin` etc.). We define a test function $f(x_1,x_2) = [x_1 \\exp(x_2), x_1+x_2]$ using $\\exp$ and show that it can be differentiated. Note that the derivative matrix is $\\begin{bmatrix} \\exp(x_2) & x_1 \\exp(x_2) \\\\ 1 & 1 \\end{bmatrix}$."
      ],
      "id": "a0488e28-7322-46ca-a2a3-6f2b9e8e81d0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e99c6fd-109d-42ed-8e2a-096d103ea00a",
        "outputId": "69fb15e2-903e-4739-d9f7-f13d602557af"
      },
      "source": [
        "def f(x):\n",
        "    return jnp.array([x[0] * jnp.exp(x[1]), x[0] + x[1]])\n",
        "\n",
        "print(\"values:\")\n",
        "print(f(jnp.array([2.0,1.0])))\n",
        "\n",
        "Df = jacfwd(f)\n",
        "print(\"jacobian matrix:\")\n",
        "print(Df(jnp.array([2.0,1.0])))\n"
      ],
      "id": "4e99c6fd-109d-42ed-8e2a-096d103ea00a",
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "values:\n",
            "[5.4365635 3.       ]\n",
            "jacobian matrix:\n",
            "[[2.7182817 5.4365635]\n",
            " [1.        1.       ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dbb295f-5416-4cc4-b59b-0da5a3abfc4c"
      },
      "source": [
        "# Exercise 2 (rootfinding with automatic differentiation, 3 points)"
      ],
      "id": "9dbb295f-5416-4cc4-b59b-0da5a3abfc4c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71362439-2461-4dd2-ba65-ee58dd4acf98"
      },
      "source": [
        "## (a)\n",
        "Create a Python function to apply Newton's method in multiple dimensions. Create a stopping criterion, such that your method automatically stops when one of the following conditions is satisfied: (i) the size of the function is below a specified tolerance; (ii) the difference in two subsequent iterates $\\mathbf{x}_k$ is below a specified tolerance; (iii) the number of iterations reaches a specified limit."
      ],
      "id": "71362439-2461-4dd2-ba65-ee58dd4acf98"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bd2d71a-e830-4636-a052-c3b455a0ab99"
      },
      "source": [
        "## (b)\n",
        "Solve Computer Exercise 5.19 using Newton's method and automatic differentiation. (N.B. Do not choose the starting point equal to a solution.)"
      ],
      "id": "4bd2d71a-e830-4636-a052-c3b455a0ab99"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a3e752a-4e45-40b2-b1f2-75a17d311a4f"
      },
      "source": [
        "# your answer here"
      ],
      "id": "0a3e752a-4e45-40b2-b1f2-75a17d311a4f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb585a79-8d3c-4d8e-9889-ddca952dbb85"
      },
      "source": [
        "Explanation using $\\LaTeX$ here"
      ],
      "id": "eb585a79-8d3c-4d8e-9889-ddca952dbb85"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36b83806-20bd-4c7f-9c8d-5bb5de23c231"
      },
      "source": [
        "# Exercise 3 (3 points)\n",
        "\n",
        "## (a)\n",
        "\n",
        "Consider the system \n",
        "$$\\begin{aligned}(x_1+3)(x_2^3-7) + 18 = {}& 0 \\\\\n",
        "\\sin(x_2 e^{x_1} -1) = {}& 0 .\n",
        "\\end{aligned}$$\n",
        "Solve this system using Newton's method with starting \n",
        "point $\\mathbf{x}_0 = [ 0.5 \\;\\; 1.4 ] ^T$.\n"
      ],
      "id": "36b83806-20bd-4c7f-9c8d-5bb5de23c231"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69553d80-f22d-46ab-ba2e-60efa972e609"
      },
      "source": [
        "# your answer here"
      ],
      "id": "69553d80-f22d-46ab-ba2e-60efa972e609",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cb2fd2b-66b7-4bdc-8793-4f3064d2339f"
      },
      "source": [
        "## (b)\n",
        "Write a program based on Broyden's method to solve the same system with the same starting point."
      ],
      "id": "6cb2fd2b-66b7-4bdc-8793-4f3064d2339f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05feae8e-0248-47a8-bb51-7b886f4f6e7d"
      },
      "source": [
        "# your answer here"
      ],
      "id": "05feae8e-0248-47a8-bb51-7b886f4f6e7d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7063fe1-dab1-41d1-85e5-875a98c2f24c"
      },
      "source": [
        "## (c)\n",
        "Compare the convergence rates of the two methods by computing the error at each iteration and appropriately analysing these errors, given that the exact solution is $\\mathbf{x}^* = [ 0 \\;\\; 1 ]^T$."
      ],
      "id": "d7063fe1-dab1-41d1-85e5-875a98c2f24c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10e04b39-1da7-4882-91a0-a649b271bfc0"
      },
      "source": [
        "# your answer here"
      ],
      "id": "10e04b39-1da7-4882-91a0-a649b271bfc0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c67ed772-59bf-43fa-92c5-b99384421b61"
      },
      "source": [
        "Explanation using $\\LaTeX$ here"
      ],
      "id": "c67ed772-59bf-43fa-92c5-b99384421b61"
    }
  ]
}